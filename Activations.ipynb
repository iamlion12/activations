{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции активации  в нейронных сетях\n",
    "====================\n",
    "\n",
    "Согласно Википедии - В нейронных сетях, функция активации нейрона определяет выход нейрона из входного значения.\n",
    "Чтобы разобраться, что и как проиходит внутри нейросети, а также как влияет функция активации на выходные значения, следует определить следующие понятия:\n",
    "* **Входные значения/Inputs/x** - в самом начале, это те, данные на основе которых следует выполнить предсказание для выходного значений.\n",
    "* **Выходные значения/Целевая переменная/Outputs/y** - те значение, которые необходимо предсказать.\n",
    "* **Веса/weights/w** - каждому входному х соответствуют какие-то w, которые инициализируются перед обучением, и оптимизируются в процессе обучения. Количество весов для каждого нейрона, зависит от количества x.\n",
    "* **Отступ/bias/b** - Аналогично весам, инициализируется до обучения и оптимизируется в процессе обучения, но в отличие от весов, bias - это одно число. \n",
    "* **Сумматорая функция $S$** - функция вида $xw+b$, где w - weights для входов x, b - bias.\n",
    "* **Функция активации $A$** - функция, которая применяется к сумматорной, задается до обучения сети, на этапе построения модели.  \n",
    "\n",
    "Для обычной нейронной сети, с прямым распространением(сигналов(которые на самом деле, какие-то числа)) с 1 нейроном в скрытом слое, последовательность \"предсказывания\"(вычисления) выходного значения такая:\n",
    "1. Взять $x$, применить к нему сумматорную функцию, с весами и отступом для скрытого нейрона.\n",
    "2. К полученному значению функцию активации.\n",
    "3. Применить к значению из пункта 2 сумматорную функцию, с весами и отступом на выходном слое.\n",
    "4. Финальный этап: применить функцию активации на выходном слое.  \n",
    "  \n",
    "Функция активации играет важнейшую роль, какую именно рассмотрим ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "from ipywidgets import interactive, interact, FloatSlider as FS, HBox, VBox, Dropdown, Tab\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Устанавлием параметры графика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 13, 6\n",
    "\n",
    "def interact_hookup(f, controls):\n",
    "    from ipywidgets import Output\n",
    "    out = Output()\n",
    "    def observer(change):\n",
    "        out.clear_output()\n",
    "        kwargs = {k:v.value for k,v in controls.items()}\n",
    "        with out:\n",
    "            f(**kwargs)\n",
    "    for k,w in controls.items():\n",
    "        w.observe(observer, 'value')\n",
    "    observer(None)\n",
    "    return out\n",
    "\n",
    "def slider(v, desc):\n",
    "    return FS(value=v, min=v-10, max=v+10, description=desc)\n",
    "\n",
    "def slider_wr(v, desc, rn, rx):\n",
    "    return FS(value=v, min=rn, max=rx, description=desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Какие бывают функции активации:\n",
    "\n",
    "**Linear** - обычная линейная функция $y=kx+b$, где $k=w$ - weight(вес нейрона), $b = bias$ - отступ,\n",
    "формула: $$A(X)=weights*X+bias$$\n",
    "В какой-то степени, не является активационной функцией вовсе, так как она же и есть сумматорная функция. Исходя из этого, когда **говорят, что функция активации линейная** - это значит функция активации не применяется вовсе. Линейную функцию активации, применяют там, где зависимость между входными данными и целевой переменной линейная или данные линейно разделимы(для задачи классификации).\n",
    "\n",
    "**Sigmoid(Logistic) a.k.a. Soft step** - функция вида: $$Sigmoid(x)=\\frac{1}{1+e^{-x}} $$\n",
    "Область значений для этой функции $(0;1)$. В сигмойде, для $x<\\pm 6$ выходные значения будут: \n",
    "$$\\begin{cases} 1, & \\mbox{если } x > 0 \\\\ 0, & \\mbox{если } x > 0 \\end{cases}$$\n",
    "В случае, если во входных данных $x>\\pm 6$, то применяя сигмойду на этих значениях, теряется информативность, в виду того что, они либо обнуляются, либо равны 1. Во избежании этого, необходимо масштабировать входные значения, то есть привести к меньшему промежутку, например разделить на максимальный возможный x из обучающих данных.\n",
    "\n",
    "**TanH** - [функция гиперболического тангенса](https://en.wikipedia.org/wiki/Hyperbolic_function#Hyperbolic_tangent), в отличии от сигмоиды, может принимать значения на отрезке $(-1;1)$. Для этой функции активации, веса должны быть инициализированы малыми случайными значениями. $$tanh(x) = \\frac{2}{1+e^{-2x}} - 1$$\n",
    "С тангесом, промежуток потери информации, ещё меньше, поэтому необходимо \"пошаманить\" над данными, также как и с сигмойдой.\n",
    "\n",
    "**ReLU - Rectified Linear Unit** - дословный перевод: Выпрямленный линейный нейрон. \n",
    "$$ReLU(x)=max(0,x)$$\n",
    "То есть, если $x \\in (-\\infty , 0)$, то $ReLU(x)=0$, иначе $ReLU(x)=x$  \n",
    "Наиболее популярная функция активации, однако, имеет ряд недостатков: \n",
    "* Недифференцируема в нуле, но диффиринцируемая сколь угодно близко к нулю, но не в самом нуле.\n",
    "* Нейроны с активацией ReLU могут умереть: случается так, что при определённых значениях весов, выходные значения сети обнуляются, то есть $ReLU(x)=0, \\forall x \\in Inputs$\n",
    "\n",
    "#### Почему ReLU популярна, хотя имеет ряд недостатков?\n",
    "Она хорошо себя показывает на практике, но при обучении, <font color=\"red\">**не следует**</font> устанавливать большой learning rate.\n",
    "Также, проблемы ReLU, решаются следующими изменениями:\n",
    "\n",
    "**Elu** - Exponential Linear Unit - улучшенная функция ReLU, записывается она так:\n",
    "$$ELU(x)= \\begin{cases} x, & \\mbox{если } x \\geq 0 \\\\ \\alpha*(e^x-1), & \\mbox{если } x <0 \\end{cases}$$\n",
    "По-умолчанию, $\\alpha=1$.\n",
    "\n",
    "**Leaky ReLU** - функция активации представленная в 2014 году, авторы которой: *Andrew L. Maas, Awni Y. Hannun, Andrew Y. Ng(Очень популярный человек в машинном обучении)*, призванная устранить проблемы ReLU.\n",
    "$${Leaky \\space ReLU}(x)=\\begin{cases} x, & \\mbox{если } x>0 \\\\ 0.01x, & \\mbox иначе \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функции активации на практике:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return 2/(1+np.exp(-2*x)) - 1\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def leaky_relu(x):\n",
    "    return np.array([0.01*v if v<=0 else v for v in x])\n",
    "\n",
    "def elu(x, alpha=1):\n",
    "    return np.array([alpha*(np.exp(v)-1) if v<=0 else v for v in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Графики:\n",
    "\n",
    "Можно выбрать функцию активации из выпадающего списка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17cf5ee7bebc4309bd8cc88185f59311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f(activation=(sigmoid, 'Sigmoid')):\n",
    "    x = np.linspace(-7,5,100)\n",
    "    plt.plot(x, activation[0](x), label='{0} activation'.format(activation[1]))\n",
    "    plt.grid()\n",
    "    plt.title('{0} function'.format(str(activation[1])))\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "interactive_plot = interactive(f,activation={'Sigmoid':(sigmoid, 'Sigmoid'), 'TanH':(tanh,'TanH'),\n",
    "                                             'ReLU':(relu,'ReLU'), 'ELU':(elu,'ELU'), \n",
    "                                             'Leaky ReLU':(leaky_relu,'Leaky ReLU')})\n",
    "output = interactive_plot.children[-1]\n",
    "display(interactive_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь представлен выход Feed-Forward Neural Network с 1 нейроном, то есть:\n",
    "$$A(S)$$\n",
    "Ползунки позволяют менять значение w(вес нейрона) и b(bias, отступ нейрона), и посмотреть как изменяется выход сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639cab4166ab4aa8ab5d42c81c96e88d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f(activation=(sigmoid, 'Sigmoid'), w=2, b=1):\n",
    "    plt.figure(2)\n",
    "    x = np.linspace(-5, 5, num=1000)\n",
    "    plt.plot(x, activation[0](x*w+b), label='w={0}, b={1}'.format(w,b))\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.title('{0} neuron'.format(str(activation[1])))\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "interactive_plot = interactive(f,activation={'Sigmoid':(sigmoid, 'Sigmoid'), 'TanH':(tanh,'TanH'),\n",
    "                                             'ReLU':(relu,'ReLU'), 'ELU':(elu,'ELU'), \n",
    "                                             'Leaky ReLU':(leaky_relu,'Leaky ReLU')},\n",
    "                                w=(-50.0, 50.0), b=(-10, 10))\n",
    "output = interactive_plot.children[-1]\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "График аналогичный предыдущему, только теперь в НН 1 скрытый слой с 2-мя нейронами, на выходе требуется 1 значение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> **Таблица для аппроскимации функции $y=x^2$** </center>  \n",
    "\n",
    "|       | Sigmoid | TanH  |  ReLU |  ELU  | Leaky ReLU |\n",
    "|:-----:|---------|-------|-------|-------|:----------:|\n",
    "| $w_1$ |  0.30   | 0.66  |  1.40 | 1.30  |    0.80    |\n",
    "| $b_1$ | -3.45   | -0.90 | -0.90 | -1.90 |    -0.40   |\n",
    "| $w_2$ |  0.30   | -0.66 | -1.40 | -1.30 |    -0.80   |\n",
    "| $b_2$ | -3.45   | -0.90 | -0.90 | -1.90 |    -0.40   |\n",
    "| $w_3$ | 300.00  | 5.00  |  1.80 | 3.00  |    3.50    |\n",
    "| $b_3$ |-11.50   | 7.00  |  0.00 | 5.11  |    0.00    |\n",
    "| $w_4$ | 300.00  | 5.00  |  1.80 | 3.00  |    3.50    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "478a5a542d514a9ea86dc1a98ebf5bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f(w1, b1, w2, b2, w3, b3, w4, activation=(sigmoid, 'Sigmoid')):\n",
    "    \n",
    "    plt.figure(2)\n",
    "    \n",
    "    x = np.linspace(-10, 10, num=1000)\n",
    "    \n",
    "    A1, A2 = activation[0](x*w1+b1),  activation[0](x*w2+b2)\n",
    "    \n",
    "    A3 = A1*w3 + A2*w4 + b3\n",
    "    \n",
    "    plt.plot(x, A1, linestyle='--', label='First neuron activation, w={0}, b={1}'.format(w1,b1))\n",
    "    plt.plot(x, A2, linestyle='--', label='Second neuron activation, w={0}, b={1}'.format(w2,b2))\n",
    "    \n",
    "    plt.plot(x, A1*w3 + b3, linestyle='--', label='First neuron output, w={0}, b={1}'.format(w3,b3))\n",
    "    plt.plot(x, A2*w4 + b3, linestyle='--', label='Second neuron output, w={0}, b={1}'.format(w4,b3))\n",
    "    \n",
    "    plt.plot(x, A3, color='r', linewidth=5.0, label='Output value')\n",
    "    plt.plot(x, x**2, linewidth=3.0, linestyle=':', label=r'$y=x^2$ function')\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.ylim(-1, 100)\n",
    "    plt.title('1 hidden layer, with 2 {0} neurons'.format(str(activation[1])))\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "acts = Dropdown(options={'Sigmoid':(sigmoid, 'Sigmoid'), 'TanH':(tanh,'TanH'),\n",
    "                         'ReLU':(relu,'ReLU'), 'ELU':(elu,'ELU'), \n",
    "                         'Leaky ReLU':(leaky_relu,'Leaky ReLU')}, \n",
    "                          description='Activation: ')\n",
    "    \n",
    "params = dict(activation = acts,   w1=slider(0.31,'w1:'), w2=slider(-.31,'w2:'), \n",
    "              w3=slider_wr(300,'w3:',-300,300), w4=slider_wr(300,'w4:',-300,300), \n",
    "              b1=slider(-3.59,'b1:'), b2=slider(-3.59,'b2:'),  b3=slider(-11.5,'b3:'))  \n",
    "\n",
    "output = interact_hookup(f, params)\n",
    "ok = VBox([params['activation'],\n",
    "           HBox([params['w1'], params['w2']]),  \n",
    "           HBox([params['w3'], params['w4']]),\n",
    "           HBox([params['b1'], params['b2']]),\n",
    "           params['b3'],output])\n",
    "display(ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из примера, ясно:\n",
    "* 2 сигмоидных нейрона, в точности повторяют $x^2$\n",
    "* ELU засчет изгиба, для $x<0$, очень хорошо справляются с задачей\n",
    "* TanH требует тонкой настройки параметров\n",
    "* ReLU и Leaky ReLU справляются с задачей хуже из-за отсутствия нелинейности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 скрытый слой, с 3-мя нейронами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962fbe63e845499584f92c71078f10d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f(activation=(sigmoid, 'Sigmoid'), **kwargs):\n",
    "    \n",
    "    plt.figure(2)\n",
    "    \n",
    "    x = np.linspace(-10, 10, num=1000)\n",
    "    \n",
    "    A1 = activation[0](x*kwargs['w1']+kwargs['b1'])\n",
    "    A2 = activation[0](x*kwargs['w2']+kwargs['b2'])\n",
    "    A3 = activation[0](x*kwargs['w3']+kwargs['b3'])\n",
    "    \n",
    "    A4 = A1*kwargs['w4'] + A2*kwargs['w5'] + A3*kwargs['w6'] + kwargs['b4']\n",
    "    \n",
    "    plt.plot(x, A1, linestyle='--', label='1st neuron activation')\n",
    "    plt.plot(x, A2, linestyle='--', label='2nd neuron activation')\n",
    "    plt.plot(x, A3, linestyle='--', label='3rd neuron activation')\n",
    "    plt.plot(x, A4, color='r', linewidth=5.0, label='Output value')\n",
    "    plt.plot(x, np.sin(x), linewidth=3.0, linestyle=':', label=r'$sin(x)$')\n",
    "    plt.grid()\n",
    "    plt.ylim(-1, 5)\n",
    "    plt.title('1 hidden layer, with 3 {0} neurons'.format(str(activation[1])))\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "params = dict(activation = acts, w1=slider( 1.35,'w1:'), w2=slider(-1.35,'w2:'), w3=slider(  1, 'w3:'),  \n",
    "              w4=slider(  1, 'w4:'), w5=slider(  1, 'w3:'),  w6=slider(  1, 'w4:'), b1=slider(-2.5, 'b1:'),\n",
    "              b2=slider(-2.5, 'b2:'), b3=slider(-1.5, 'b3:'),  b4=slider(-1.5, 'b3:'))  \n",
    "\n",
    "output = interact_hookup(f, params)\n",
    "ok = VBox([params['activation'],\n",
    "           HBox([params['w1'], params['w2'], params['w3']]),  \n",
    "           HBox([params['w4'], params['w5'], params['w6']]),\n",
    "           HBox([params['b1'], params['b2'], params['b3'], params['b4']]),\n",
    "           output])\n",
    "display(ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 скрытых слоя, по 3 нейрона в каждом \n",
    "\n",
    "### $W{_i}{^j}$ - $i$ - слой, $j$ - индекс нейрона"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b67de1c2b64689ab964dff3aa60197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f(activation=(sigmoid, 'Sigmoid'), **kwargs):\n",
    "    \n",
    "    plt.figure(2)\n",
    "    x = np.linspace(-10, 10, num=1000)\n",
    "    \n",
    "    A11 = activation[0](x*kwargs['w11']+kwargs['b11'])\n",
    "    A12 = activation[0](x*kwargs['w12']+kwargs['b12'])\n",
    "    A13 = activation[0](x*kwargs['w13']+kwargs['b13'])\n",
    "    \n",
    "    A21 = activation[0](A11*kwargs['w21'] + A12*kwargs['w22'] + A13*kwargs['w23'] + kwargs['b21'])\n",
    "    A22 = activation[0](A11*kwargs['w24'] + A12*kwargs['w25'] + A13*kwargs['w26'] + kwargs['b22'])\n",
    "    A23 = activation[0](A11*kwargs['w27'] + A12*kwargs['w28'] + A13*kwargs['w29'] + kwargs['b23'])\n",
    "    \n",
    "    A3 = kwargs['w31']*A21+kwargs['w32']*A22+kwargs['w33']*A23+kwargs['b3']\n",
    "    \n",
    "    plt.plot(x, A11, linestyle='--', label='1st layer 1 neuron activation')\n",
    "    plt.plot(x, A12, linestyle='--', label='1st layer 2 neuron activation')\n",
    "    plt.plot(x, A13, linestyle='--', label='1st layer 3 neuron activation')\n",
    "    \n",
    "    plt.plot(x, A21, linestyle='--', label='2nd layer 1 neuron activation')\n",
    "    plt.plot(x, A22, linestyle='--', label='2nd layer 2 neuron activation')\n",
    "    plt.plot(x, A23, linestyle='--', label='2nd layer 3 neuron activation')\n",
    "    \n",
    "    plt.plot(x, A3, color='r', linewidth=5.0, label='Model output value')\n",
    "    \n",
    "    plt.plot(x, x**2, linewidth=3.0, linestyle=':', label='sqr(x) function')\n",
    "    plt.grid()\n",
    "    plt.ylim(-1, 5)\n",
    "    plt.title('Model with 2 hidden layer, each have 3 {0} neurons'.format(activation[1]))\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "params = dict(activation = acts, w11=slider(1,'w11'),w12=slider(1,'w12'),w13=slider(1,'w13'),\n",
    "              w21=slider(1,'w21'),w22=slider(1,'w22'),w23=slider(1,'w23'),w24=slider(1,'w24'),\n",
    "              w25=slider(1,'w25'),w26=slider(1,'w26'),w27=slider(1,'w27'),w28=slider(1,'w28'),\n",
    "              w29=slider(1,'w29'),w31=slider(1,'w31'),w32=slider(1,'w32'),w33=slider(1,'w33'),\n",
    "              b11=slider(1,'b11'),b12=slider(1,'b12'),b13=slider(1,'b13'),b21=slider(1,'b21'),\n",
    "              b22=slider(1,'b22'),b23=slider(1,'b23'),b3=slider(1,'b3')) \n",
    "\n",
    "tab = Tab()\n",
    "tab.children = [VBox([HBox([params['w11'],params['w12'],params['w13']]),\n",
    "                      HBox([params['b11'],params['b12'],params['b13']])]),\n",
    "                VBox([HBox([params['w21'],params['w22'],params['w23'],params['w24']]),\n",
    "                      HBox([params['w25'],params['w26'],params['w27'],params['w28']]),\n",
    "                      HBox([params['w29'],params['b21'],params['b22'],params['b23']])]),\n",
    "                HBox([params['w31'],params['w32'],params['w33'],params['b3']])]\n",
    "\n",
    "for i in range(len(tab.children)):\n",
    "    tab.set_title(i, \"{0} layer parameters\".format(str(i+1)))\n",
    "\n",
    "output = interact_hookup(f, params)\n",
    "ok = VBox([params['activation'], tab, output])\n",
    "display(ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Заключение:  \n",
    "Нейронная сеть, может всё. Но следует тщательно подбирать активационную функцию. Также необходимо проводить предобработку данных, перед обучением.\n",
    "\n",
    "\n",
    "<div style=\"text-align: right\">**made by <font color=\"orange\">[@iamlion12](https://t.me/iamlion12)</font>**</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
