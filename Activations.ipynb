{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции активации  в нейронных сетях\n",
    "====================\n",
    "\n",
    "Согласно Википедии - В нейронных сетях, функция активации нейрона определяет выход нейрона из входного значения.\n",
    "Чтобы разобраться, что и как проиходит внутри нейросети, а также как влияет функция активации на выходные значения, следует определить следующие понятия:\n",
    "* **Входные значения/Inputs/x** - в самом начале, это те, данные на основе которых следует выполнить предсказание для выходного значений.\n",
    "* **Входные значения/Inputs/y** - те значение, которые необходимо предсказать\n",
    "* **Веса/weights/w** - каждому входному х соответствуют какие-то w, которые инициализируются перед обучением, и оптимизируются в процессе обучения. Количество весов для каждого нейрона, зависит от количества x.\n",
    "* **Отступ/bias/b** - Аналогично весам, инициализируется до обучения и оптимизируется в процессе обучения, но в отличие от весов, bias - это одно число. \n",
    "* **Сумматорая функция $S$** - функция вида $xw+b$, где w - weights для входов x, b - bias.\n",
    "* **Функция активации $A$** - функция, которая применяется к сумматорной, задается до обучения сети, на этапе построения модели.  \n",
    "\n",
    "Для обычной нейронной сети, с прямым распространением(сигналов(которые на самом деле, какие-то числа)) с 1 нейроном в скрытом слое, последовательность \"предсказывания\"(вычисления) выходного значения такая:\n",
    "1. Взять $x$, применить к нему сумматорную функцию, с весами и отступом для скрытого нейрона.\n",
    "2. К полученному значению функцию активации.\n",
    "3. Применить к значению из пункта 2 сумматорную функцию, с весами и отступом на выходном слое.\n",
    "4. Финальный этап: применить функцию активации на выходном слое.  \n",
    "  \n",
    "Функция активации играет важнейшую роль, какую именно рассмотрим ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "from ipywidgets import interactive, interact, FloatSlider, HBox, VBox, Dropdown\n",
    "import ipywidgets as widgets\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Устанавлием параметры графика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 13, 6\n",
    "\n",
    "def interact_hookup(f, controls):\n",
    "    from ipywidgets import Output\n",
    "    out = Output()\n",
    "    def observer(change):\n",
    "        out.clear_output()\n",
    "        kwargs = {k:v.value for k,v in controls.items()}\n",
    "        with out:\n",
    "            f(**kwargs)\n",
    "    for k,w in controls.items():\n",
    "        w.observe(observer, 'value')\n",
    "    observer(None)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Какие бывают функции активации:\n",
    "\n",
    "**Linear** - обычная линейная функция $y=kx+b$, где $k=w$ - weight(вес нейрона), $b = bias$ - отступ,\n",
    "формула: $$A(X)=weights*X+bias$$\n",
    "В какой-то степени, не является активационной функцией вовсе, так как она же и есть сумматорная функция. Исходя из этого, когда **говорят, что функция активации линейная** - это значит функция активации не применяется вовсе. Линейную функцию активации, применяют там, где зависимость между входными данными и целевой переменной линейная или данные линейно разделимы(для задачи классификации).\n",
    "\n",
    "**Sigmoid(Logistic) a.k.a. Soft step** - функция вида: $$Sigmoid(x)=\\frac{1}{1+e^{-x}} $$\n",
    "Область значений для этой функции $(0;1)$. В сигмойде, для $x<\\pm 6$ выходные значения будут: \n",
    "$$\\begin{cases} 1, & \\mbox{если } x > 0 \\\\ 0, & \\mbox{если } x > 0 \\end{cases}$$\n",
    "В случае, если во входных данных $x>\\pm 6$, то применяя сигмойду на этих значениях, теряется информативность, в виду того что, они либо обнуляются, либо равны 1. Во избежании этого, необходимо масштабировать входные значения, то есть привести к меньшему промежутку, например разделить на максимальный возможный x из обучающих данных.\n",
    "\n",
    "**TanH** - [функция гиперболического тангенса](https://en.wikipedia.org/wiki/Hyperbolic_function#Hyperbolic_tangent), в отличии от сигмоиды, может принимать значения на отрезке $(-1;1)$. Для этой функции активации, веса должны быть инициализированы малыми случайными значениями. $$tanh(x) = \\frac{2}{1+e^{-2x}} - 1$$\n",
    "С тангесом, промежуток потери информации, ещё меньше, поэтому необходимо \"пошаманить\" над данными, также как и с сигмойдой.\n",
    "\n",
    "**ReLU - Rectified Linear Unit** - дословный перевод: Выпрямленный линейный нейрон. \n",
    "$$ReLU(x)=max(0,x)$$\n",
    "То есть, если $x \\in (-\\infty , 0)$, то $ReLU(x)=0$, иначе $ReLU(x)=x$  \n",
    "Наиболее популярная функция активации, однако, имеет ряд недостатков: \n",
    "* Недифференцируема в нуле, но диффиринцируемая сколь угодно близко к нулю, но не в самом нуле.\n",
    "* Нейроны с активацией ReLU могут умереть: случается так, что при определённых значениях весов, выходные значения сети обнуляются, то есть $ReLU(x)=0, \\forall x \\in Inputs$\n",
    "\n",
    "#### Почему ReLU популярна, хотя имеет ряд недостатков?\n",
    "Она хорошо себя показывает на практике, но при обучении, <font color=\"red\">**не следует**</font> устанавливать большой learning rate.\n",
    "Также, проблемы ReLU, решаются следующими изменениями:\n",
    "\n",
    "**Elu** - Exponential Linear Unit - улучшенная функция ReLU, записывается она так:\n",
    "$$ELU(x)= \\begin{cases} x, & \\mbox{если } x \\geq 0 \\\\ \\alpha*(e^x-1), & \\mbox{если } x <0 \\end{cases}$$\n",
    "По-умолчанию, $\\alpha=1$.\n",
    "\n",
    "**Leaky ReLU** - функция активации представленная в 2014 году, авторы которой: *Andrew L. Maas, Awni Y. Hannun, Andrew Y. Ng(Очень популярный человек в машинном обучении)*, призванная устранить проблемы ReLU.\n",
    "$${Leaky \\space ReLU}(x)=\\begin{cases} x, & \\mbox{если } x>0 \\\\ 0.01x, & \\mbox иначе \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функции активации на практике:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return 2/(1+np.exp(-2*x)) - 1\n",
    "\n",
    "def relu(x):\n",
    "    return max(0,x)\n",
    "\n",
    "def leaky_relu(x):\n",
    "    return x if x > 0 else 0.01*x\n",
    "\n",
    "def elu(x, alpha=1):\n",
    "    return x if x > 0 else alpha*(np.exp(x)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Графики:\n",
    "\n",
    "Можно выбрать функцию активации из выпадающего списка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a571ef1af649d49f9dff635096d715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f(activation=(sigmoid, 'Sigmoid')):\n",
    "    x = np.linspace(-7,5,100)\n",
    "    plt.plot(x, list(map(activation[0], x)), label='{0} activation'.format(activation[1]))\n",
    "    plt.grid()\n",
    "    plt.title('{0} function'.format(str(activation[1])))\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "interactive_plot = interactive(f,activation={'Sigmoid':(sigmoid, 'Sigmoid'), 'TanH':(tanh,'TanH'),\n",
    "                                             'ReLU':(relu,'ReLU'), 'ELU':(elu,'ELU'), \n",
    "                                             'Leaky ReLU':(leaky_relu,'Leaky ReLU')})\n",
    "output = interactive_plot.children[-1]\n",
    "display(interactive_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь представлен выход Feed-Forward Neural Network с 1 нейроном, то есть:\n",
    "$$A(S)$$\n",
    "Ползунки позволяют менять значение w(вес нейрона) и b(bias, отступ нейрона), и посмотреть как изменяется выход сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403fc8a1dfd24f16956b73d56fbc8211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f(activation=(sigmoid, 'Sigmoid'), w=2, b=1):\n",
    "    plt.figure(2)\n",
    "    x = np.linspace(-5, 5, num=1000)\n",
    "    plt.plot(x, list(map(activation[0], x*w+b)), label='w={0}, b={1}'.format(w,b))\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.title('{0} neuron'.format(str(activation[1])))\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "interactive_plot = interactive(f,activation={'Sigmoid':(sigmoid, 'Sigmoid'), 'TanH':(tanh,'TanH'),\n",
    "                                             'ReLU':(relu,'ReLU'), 'ELU':(elu,'ELU'), \n",
    "                                             'Leaky ReLU':(leaky_relu,'Leaky ReLU')},\n",
    "                                w=(-50.0, 50.0), b=(-10, 10))\n",
    "output = interactive_plot.children[-1]\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "График аналогичный предыдущему, только теперь в НН 1 скрытый слой с 2-мя нейронами, на выходе требуется 1 значение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> **Таблица для аппроскимации фукнции $y=x^2$** </center>  \n",
    "\n",
    "|       | Sigmoid | TanH  |  ReLU |  ELU  | Leaky ReLU |\n",
    "|:-----:|---------|-------|-------|-------|:----------:|\n",
    "| $w_1$ |  1.30   | 0.66  |  1.40 | 1.30  |    0.80    |\n",
    "| $b_1$ | -2.50   | -0.90 | -0.90 | -1.90 |    -0.40   |\n",
    "| $w_2$ | -1.30   | -0.66 | -1.40 | -1.30 |    -0.80   |\n",
    "| $b_2$ | -2.50   | -0.90 | -0.90 | -1.90 |    -0.40   |\n",
    "| $w_3$ |  10.00  | 5.00  |  1.80 | 3.00  |    3.50    |\n",
    "| $b_3$ | -1.50   | 7.00  |  0.00 | 5.00  |    0.00    |\n",
    "| $w_4$ |  10.00  | 5.00  |  1.80 | 3.00  |    3.50    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cae94d8021a4d0ba38c834093d5a7a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f(w1, b1, w2, b2, w3, b3, w4, activation=(sigmoid, 'Sigmoid')):\n",
    "    plt.figure(2)\n",
    "    x = np.linspace(-10, 10, num=1000)\n",
    "    A1 = np.array(list(map(activation[0], x*w1+b1)))\n",
    "    A2 = np.array(list(map(activation[0], x*w2+b2)))\n",
    "    A3 = A1*w3 + A2*w4 + b3\n",
    "    plt.plot(x, A1, linestyle='--', label='First neuron activation, w={0}, b={1}'.format(w1,b1))\n",
    "    plt.plot(x, A2, linestyle='--', label='Second neuron activation, w={0}, b={1}'.format(w2,b2))\n",
    "    \n",
    "    plt.plot(x, A1*w3 + b3, linestyle='--', label='First neuron output, w={0}, b={1}'.format(w3,b3))\n",
    "    plt.plot(x, A2*w4 + b3, linestyle='--', label='Second neuron output, w={0}, b={1}'.format(w4,b3))\n",
    "    \n",
    "    plt.plot(x, A3, color='r', linewidth=5.0, \n",
    "             label='Output value')\n",
    "    plt.plot(x, x**2, linewidth=3.0, linestyle=':', label=r'$y=x^2$ function')\n",
    "    plt.grid()\n",
    "    plt.ylim(-1, 5)\n",
    "    plt.title('1 hidden layer, with 2 {0} neurons'.format(str(activation[1])))\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "params = dict(activation = Dropdown(options={'Sigmoid':(sigmoid, 'Sigmoid'), 'TanH':(tanh,'TanH'),\n",
    "                                             'ReLU':(relu,'ReLU'), 'ELU':(elu,'ELU'), \n",
    "                                             'Leaky ReLU':(leaky_relu,'Leaky ReLU')}, \n",
    "                                              description='Activation: '),\n",
    "                           w1=FloatSlider(value= 1.3, min=-5, max=11, description='w1:'),\n",
    "                           w2=FloatSlider(value=-1.3, min=-5, max=11, description='w2:'),\n",
    "                           w3=FloatSlider(value=  10, min=-10,max=11, description='w3:'),\n",
    "                           w4=FloatSlider(value=  10, min=-10,max=11, description='w4:'),\n",
    "                           b1=FloatSlider(value=-2.5, min=-5, max=11, description='b1:'),\n",
    "                           b2=FloatSlider(value=-2.5, min=-5, max=11, description='b2:'),\n",
    "                           b3=FloatSlider(value=-1.5, min=-5, max=11, description='b3:'))  \n",
    "\n",
    "output = interact_hookup(f, params)\n",
    "ok = VBox([params['activation'],\n",
    "           HBox([params['w1'], params['w2']]),  \n",
    "           HBox([params['w3'], params['w4']]),\n",
    "           HBox([params['b1'], params['b2']]),\n",
    "           params['b3'],output])\n",
    "display(ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из примера становиться ясно:\n",
    "* 2 сигмоидных нейрона, в точности повторяют $x^2$\n",
    "* ELU засчет изгиба для $x<0$, очень хорошо справляются с задачей\n",
    "* TanH требует тонкой настройки параметров\n",
    "* ReLU и Leaky ReLU справляются с задачей хуже из-за отсутствия нелинейности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 скрытый слой, с 3-мя нейронами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "744b35a60ff14ae48744d11adf964730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f(activation=(sigmoid, 'Sigmoid'),\n",
    "                  w1=-1, w2=2, w3=0.4, w4=1,  \n",
    "                  w5=1, w6=1,\n",
    "                  b1=0, b2=0, b3=2, b4=0):\n",
    "    \n",
    "    plt.figure(2)\n",
    "    \n",
    "    x = np.linspace(-10, 10, num=1000)\n",
    "    \n",
    "    A1 = np.array(list(map(activation[0], x*w1+b1)))\n",
    "    A2 = np.array(list(map(activation[0], x*w2+b2)))\n",
    "    A3 = np.array(list(map(activation[0], x*w3+b3)))\n",
    "    \n",
    "    A4 = A1*w4 + A2*w5 + A3*w6 + b4\n",
    "    \n",
    "    plt.plot(x, A1, linestyle='--', label='1st neuron activation, w={0}, b={1}'.format(w1,b1))\n",
    "    plt.plot(x, A2, linestyle='--', label='2nd neuron activation, w={0}, b={1}'.format(w2,b2))\n",
    "    plt.plot(x, A3, linestyle='--', label='3rd neuron activation, w={0}, b={1}'.format(w3,b3))\n",
    "    plt.plot(x, A4, color='r', linewidth=5.0, \n",
    "             label='Output value, w3={0}, b3={1}, w4={0}, b4={1}'.format(w3,b3,w4,b4))\n",
    "    plt.plot(x, x**2, linewidth=3.0, linestyle=':', label='Sine function')\n",
    "    plt.grid()\n",
    "    plt.ylim(-1, 5)\n",
    "    plt.title('1 hidden layer, with 3 {0} neurons'.format(str(activation[1])))\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "interactive_plot = interactive(f,activation={'Sigmoid':(sigmoid, 'Sigmoid'), 'TanH':(tanh,'TanH'),\n",
    "                                             'ReLU':(relu,'ReLU'), 'ELU':(elu,'ELU'), \n",
    "                                             'Leaky ReLU':(leaky_relu,'Leaky ReLU')},  \n",
    "                                w1=(-5.0, 5.0), w2=(-5.0, 5.0), w3=(-5.0, 5.0), w4=(-5.0, 5.0),\n",
    "                                w5=(-5.0, 5.0), w6=(-5.0, 5.0), b1=(-5.0, 5.0), b2=(-5.0, 5.0),\n",
    "                                b3=(-5.0, 5.0), b4=(-5.0, 5.0))\n",
    "output = interactive_plot.children[-1]\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 скрытых слоя, по 3 нейрона в каждом \n",
    "\n",
    "### $W{_i}{^j}$ - $i$ - слой, $j$ - индекс нейрона"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b082c0c7eaf48b88092306cad1bb831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f(activation=(sigmoid, 'Sigmoid'),\n",
    "      w11=0.5, w12=1, w13=1,\n",
    "      w21=0.5, w22=1, w23=1, w24=1, w25=1, w26=1, w27=1, w28=1, w29=1,\n",
    "      w31=0.5, w32=1, w33=1, \n",
    "      b11=0.5, b12=0, b13=0, b21=0, b22=0, b23=0, b3=0):\n",
    "    \n",
    "    plt.figure(2)\n",
    "    x = np.linspace(-10, 10, num=1000)\n",
    "    \n",
    "    A11 = np.array(list(map(activation[0], x*w11+b11)))\n",
    "    A12 = np.array(list(map(activation[0], x*w12+b12)))\n",
    "    A13 = np.array(list(map(activation[0], x*w13+b13)))\n",
    "    \n",
    "    A21 = np.array(list(map(activation[0], A11*w21 + A12*w22 + A13*w23 + b21)))\n",
    "    A22 = np.array(list(map(activation[0], A11*w24 + A12*w25 + A13*w26 + b22)))\n",
    "    A23 = np.array(list(map(activation[0], A11*w27 + A12*w28 + A13*w29 + b23)))\n",
    "    \n",
    "    A3 = w31*A21+w32*A22+w33*A23+b3\n",
    "    \n",
    "    plt.plot(x, A11, linestyle='--', label='1st layer 1 neuron activation')\n",
    "    plt.plot(x, A12, linestyle='--', label='1st layer 2 neuron activation')\n",
    "    plt.plot(x, A13, linestyle='--', label='1st layer 3 neuron activation')\n",
    "    \n",
    "    plt.plot(x, A21, linestyle='--', label='2nd layer 1 neuron activation')\n",
    "    plt.plot(x, A22, linestyle='--', label='2nd layer 2 neuron activation')\n",
    "    plt.plot(x, A23, linestyle='--', label='2nd layer 3 neuron activation')\n",
    "    \n",
    "    plt.plot(x, A3, color='r', linewidth=5.0, label='Model output value')\n",
    "    \n",
    "    plt.plot(x, x**2, linewidth=3.0, linestyle=':', label='sqr(x) function')\n",
    "    plt.grid()\n",
    "    plt.ylim(-1, 5)\n",
    "    plt.title('Model with 2 hidden layer, each have 3 {0} neurons'.format(activation[1]))\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "interactive_plot = interactive(f,activation={'Sigmoid':(sigmoid, 'Sigmoid'), 'TanH':(tanh,'TanH'),\n",
    "                                             'ReLU':(relu,'ReLU'), 'ELU':(elu,'ELU'), \n",
    "                                             'Leaky ReLU':(leaky_relu,'Leaky ReLU')}, \n",
    "                               w11=(-10.0, 10.0),w12=(-10.0, 10.0),w13=(-10.0, 10.0),\n",
    "                               w21=(-10.0, 10.0),w22=(-10.0, 10.0),w23=(-10.0, 10.0),\n",
    "                               w24=(-10.0, 10.0),w25=(-10.0, 10.0),w26=(-10.0, 10.0),\n",
    "                               w27=(-10.0, 10.0),w28=(-10.0, 10.0),w29=(-10.0, 10.0),\n",
    "                               w31=(-10.0, 10.0),w32=(-10.0, 10.0),w33=(-10.0, 10.0),\n",
    "                               b11=(-10.0, 10.0),b12=(-10.0, 10.0),b13=(-10.0, 10.0),\n",
    "                               b21=(-10.0, 10.0),b22=(-10.0, 10.0),b23=(-10.0, 10.0),\n",
    "                               b3= (-10.0, 10.0),)\n",
    "output = interactive_plot.children[-1]\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Заключение:  \n",
    "Нейронная сеть, может всё. Но следует тщательно подбирать активационную функцию. Также необходимо проводить предобработку данных, перед обучением.\n",
    "\n",
    "\n",
    "<div style=\"text-align: right\">**made by <font color=\"orange\">[@iamlion12](https://t.me/iamlion12)</font>**</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
